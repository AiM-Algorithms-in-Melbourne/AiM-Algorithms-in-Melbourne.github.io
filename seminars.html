<!doctype html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <title>AiM Group — Seminars</title>
  <link rel="stylesheet" href="assets/css/style.css" />
</head>
<body>
  <header class="site-header">
    <div class="header-inner">
      <div class="brand" style="flex-direction:row; align-items:center; gap:10px;">
    <img src="assets/img/AiM-logo.png"
     alt="AiM Logo"
     style="
       height:140px;
       width:120px;          /* 控制裁掉多少 */
       object-fit: cover;   /* 关键 */
       object-position: center;
     " />
    <div class="title" style="font-size:26px; font-weight:600;">AiM Group (Algorithms in Melbourne)</div> 
    </div>

      <nav class="nav" aria-label="Main navigation">
        <a href="index.html" style="font-weight:600;">Home</a>
        <a href="people.html" style="font-weight:600;">People</a>
        <a class="active" href="seminars.html" style="font-weight:600;">Seminars</a>
      </nav>
    </div>
  </header>

  <main class="container">
    <section class="section">
      <h2>Seminar Series</h2>
      <p class="lead">
        We host research seminars regularly, primarily focused on algorithms and data structures. Seminars may be held in person, online, or in a hybrid format. Please find details of upcoming talks in the News section. We warmly welcome anyone who would like to give a talk. Please feel free to email Hanzhi (hanzhi.hzwang@gmail.com).</p>

        <p>We maintain a <strong>YouTube channel (<a href="https://www.youtube.com/@Algorithms_in_Melbourne" target="_blank">AiM Lab@Algorithms_in_Melbourne</a>)</strong> where we post recordings of past seminars. You are more than welcome to subscribe!</p>

        <p>We are very grateful to the speakers for allowing us to record and publish their presentations. The recorded videos are intended for learning and academic purposes only.
      </p>
    </section>

    <section class="section">
      <h2>Upcoming</h2>
      <ul class="seminar-list">
        <li>
          <span class="seminar-title">TBA</span>
          <span class="tag">On-site / Zoom</span><br/>
          <span class="small">Date: TBA • Speaker: TBA • Location: Melbourne Connect</span>
        </li>
      </ul>
    </section>

<section class="section">
<h2>Past</h2>

<div class="seminar-card two-col">
  <div class="seminar-date">
    <div class="day">20</div>
    <div class="month">Jan</div>
    <div class="year">2026</div>
  </div>

  <div class="seminar-content">
    <div class="seminar-header">
      <span class="seminar-speaker"><strong>Tim Rieder</strong></span> 
      <span class="seminar-affil">(ETH Zürich)</span>
    </div>

    <div class="seminar-title">
    Seminar Title: <strong>Random-Shift Revisited: Tight Approximations for Tree Embeddings and L1-Oblivious Routings</strong>
    <span class="seminar-links">
<!--<a href="https://www.youtube.com/watch?v=EhCHsJpsb2U" target="_blank">[Video]</a>-->
    <!--    <a href="slides/mikkel-thorup.pdf" target="_blank">[Slides]</a>-->
    </span>
    </div>
    
    <div class="seminar-host">
    Host: Dr. William Umboh
    </div>

    <details class="seminar-details">
      <summary>Abstract </summary>
      <div class="seminar-details-body">
        <p>
<!--        <strong>Abstract: </strong><br>-->
        We present a new and surprisingly simple analysis of random-shift decompositions -- originally proposed by Miller, Peng, and Xu [SPAA'13]: We show that decompositions for exponentially growing scales D=2^0,2^1,…,2^log_2(diam(G)), have a tight constant trade-off between distance-to-center and separation probability on average across the distance scales -- opposed to a necessary Ω(logn) trade-off for a single scale.

        This almost immediately yields a way to compute a tree T for graph G that preserves all graph distances with expected O(logn)-stretch. This gives an alternative proof that obtains tight approximation bounds of the seminal result by Fakcharoenphol, Rao, and Talwar [STOC'03] matching the Ω(logn) lower bound by Bartal [FOCS'96]. Our insights can also be used to refine the analysis of a simple ℓ1-oblivious routing proposed in [FOCS'22], yielding a tight O(logn) competitive ratio.

        Our algorithms for constructing tree embeddings and ℓ1-oblivious routings can be implemented in the sequential, parallel, and distributed settings with optimal work, depth, and rounds, up to polylogarithmic factors. Previously, fast algorithms with tight guarantees were not known for tree embeddings in parallel and distributed settings, and for ℓ1-oblivious routings, not even a fast sequential algorithm was known.
        </p>

        </div>
    </details>
  </div>
</div>


<div class="seminar-card two-col">
  <div class="seminar-date">
    <div class="day">22</div>
    <div class="month">Dec</div>
    <div class="year">2025</div>
  </div>

  <div class="seminar-content">
    <div class="seminar-header">
      <span class="seminar-speaker"><strong><a href="https://hjemmesider.diku.dk/~mthorup/">Prof. Mikkel Thorup</a></strong></span> 
      <span class="seminar-affil">(BARC, University of Copenhagen)</span>
    </div>

    <div class="seminar-title">
    Seminar Title: <strong>Hash functions: bridging the gap from theory to practice</strong>
    <span class="seminar-links">
    <a href="https://www.youtube.com/watch?v=EhCHsJpsb2U" target="_blank">[Video]</a>
    <!--    <a href="slides/mikkel-thorup.pdf" target="_blank">[Slides]</a>-->
    </span>
    </div>
    
    <div class="seminar-host">
    Host: Dr. Hanzhi Wang
    </div>

    <details class="seminar-details">
      <summary>Speaker Bio & Abstract </summary>
      <div class="seminar-details-body">
        <p><strong>Bio: </strong><br>
        Mikkel Thorup has a D.Phil. from Oxford University from 1993. From 1993 to 1998 he was at the University of Copenhagen. From 1998 to 2013 he was at AT&T Labs-Research. Since 2013 he has been back as Professor at the University of Copenhagen. He is currently a VILLUM Investigator heading Center for Basic Algorithms Research Copenhagen (BARC). Mikkel is a Fellow of the ACM and of AT&T, and a Member of the Royal Danish Academy of Sciences and Letters. He is co-winner of the 2011 MAA Robbins Award in mathematics and winner of the 2015 Villum Kann Rasmussen Award for Technical and Scientific Research, which is Denmark's biggest individual prize for research. More recently he was co-winner of the 2021 AMS-MOS Fulkerson Prize and an ACM STOC 20-year test of time award. Mikkel's main work is in algorithms and data structures, where he has worked on both upper and lower bounds. Recently one of his main focusses has been on hash functions unifying theory and practice. Mikkel prefers to seek his mathematical inspiration in nature, combining the quest with his hobbies of bird watching and mushroom picking.
        </p>
        
        <p><strong>Abstract: </strong><br>
        Hash functions are used everywhere in computing, e.g., hash tables, sketching, dimensionality reduction, sampling, and estimation. Abstractly, we like to think of hashing as fully-random hashing, assigning independent hash values to every possible key, but essentially this requires us to store the hash values for all keys, which is unrealistic for most key universes, e.g., 64-bit keys. In practice, we have to settle for implementable hash functions, and often practitioners settle for implementations that are too simple in that the algorithms ends up working only for sufficiently random input. However, the real world is full of structured/non-random input. For simplistic hash functions that often work very well in tests with random input, error events that should never happen in practice happen with way too high probability. 
 
        Over the last decade there has been major developments in simple-to-implement tabulation-based hash functions offering strong theoretical guarantees, so as to support fundamental properties such as Chernoff bounds, Sparse Johnson-Lindenstrauss transforms, and fully-random hashing on a given set w.h.p. etc. I will discuss some of the principles of these developments and offer insights on how far we can bridge from theory (assuming fully random hash functions) to practice (needing something that can actually implemented efficiently).
        </p>

        </div>
    </details>
  </div>
</div>

<div class="seminar-card two-col">
  <div class="seminar-date">
    <div class="day">19</div>
    <div class="month">Dec</div>
    <div class="year">2025</div>
  </div>

  <div class="seminar-content">
    <div class="seminar-header">
      <span class="seminar-speaker"><strong><a href="https://viterbi-web.usc.edu/~shanghua/">Prof. Shang-Hua Teng</a></strong></span> 
      <span class="seminar-affil">(University of Southern California)</span>
    </div>

    <div class="seminar-title">
    Seminar Title: <strong>Proper Learnability and the Role of Unlabeled Data and Regularization</strong>
    <span class="seminar-links">
    <a href="https://www.youtube.com/watch?v=zYIPNOJ3aAQ" target="_blank">[Video]</a>
    <!--    <a href="slides/mikkel-thorup.pdf" target="_blank">[Slides]</a>-->
    </span>
    </div>
    
    <div class="seminar-host">
    Host: Dr. Hanzhi Wang
    </div>

    <details class="seminar-details">
      <summary>Speaker Bio & Abstract </summary>
      <div class="seminar-details-body">
        <p><strong>Bio: </strong><br>
        Shang-Hua Teng is a USC University Professor of Computer Science and Mathematics. He is a fellow of SIAM, ACM, and Alfred P. Sloan Foundation, and has twice won the Gödel Prize, first in 2008, for developing smoothed analysis, and then in 2015, for designing the breakthrough scalable Laplacian solver. Citing him as, “one of the most original theoretical computer scientists in the world”, the Simons Foundation named him a 2014 Simons Investigator to pursue long-term curiosity-driven fundamental research. He also received the 2009 Fulkerson Prize,  2023 Science & Technology Award for Overseas Chinese from the China Computer Federation, 2025 ACM STOC Test of Time Award & 2011 ACM STOC Best Paper Award (for improving maximum-flow minimum-cut algorithms), 2022 ACM SIGecom Test of Time Award (for settling the complexity of computing a Nash equilibrium), 2021 ACM STOC Test of Time Award (for smoothed analysis), 2020 Phi Kappa Phi Faculty Recognition Award (2020) for his book Scalable Algorithms for Data and Network Analysis. In addition, he and collaborators developed the first optimal well-shaped Delaunay mesh generation algorithms for arbitrary three-dimensional domains, settled the Rousseeuw-Hubert regression-depth conjecture in robust statistics, and resolved two long-standing complexity-theoretical questions regarding the Sprague-Grundy theorem in combinatorial game theory. For his industry work with Xerox, NASA, Intel, IBM, Akamai, and Microsoft, he received fifteen patents in areas including compiler optimization, Internet technology, and social networks.
        </p>
        
        <p></p>
        <p><strong>Abstract: </strong><br>
        Proper learning refers to the setting in which learners must emit predictors in the underlying hypothesis class H, and often leads to learners with simple algorithmic forms (e.g., empirical risk minimization (ERM), regularization based structural risk minimization (SRM)). The limitation of proper learning, however, is that there exist problems which can only be learned improperly, e.g. in multiclass classification. Thus, we ask: Under what assumptions on the hypothesis class or the information provided to the learner is a problem properly learnable? We demonstrate that when the unlabeled data distribution is given, there always exists an optimal proper learner governed by distributional regularization, a randomized generalization of regularization. We refer to this setting as the distribution-fixed PAC model, and continue to evaluate the learner on its worst case performance over all distributions. </p>
        <p>
        Furthermore, we precisely characterize the role of regularization in perhaps the simplest setting for which ERM and SRM fails: multiclass learning with arbitrary label sets. Using one-inclusion graphs (OIGs), we exhibit optimal learning algorithms that dovetail with tried-and-true algorithmic principles: Occam’s Razor as embodied by structural risk minimization (SRM), the principle of maximum entropy, and Bayesian inference. We extract from OIGs a combinatorial sequence we term the Hall complexity, which is the first to characterize a problem’s transductive error rate exactly; we also introduce a generalization of OIGs and the transductive learning setting to the agnostic case. We demonstrate that an agnostic version of the Hall complexity again characterizes error rates exactly, and exhibit an optimal learner using maximum entropy programs. This work provides new insights into the role of unlabeled data, and how it can empower local regularization in machine learning. </p>
        <p>
        Joint work (COLT 2024, ALT 2025) with Julian Asilis, Siddartha Devic, Shaddin Dughmi, and Vatsal Sharan        
        </p>

        </div>
    </details>
  </div>
</div>

</section>


    <div class="footer">
      © <span id="year"></span> AiM Lab
    </div>
  </main>

  <script>
    document.getElementById("year").textContent = new Date().getFullYear();
  </script>
</body>
</html>